{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJCVQsX-cjMT"
      },
      "outputs": [],
      "source": [
        "!unzip real_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p58l4GVadw_m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def load_dna_sequences(folder):\n",
        "    sequences = []\n",
        "    for filename in sorted(os.listdir(folder)):\n",
        "        if filename.endswith('.dna'):\n",
        "            with open(os.path.join(folder, filename), 'r') as file:\n",
        "                # Read the file content\n",
        "                content = file.read()\n",
        "                # Remove whitespaces, newlines, and special characters\n",
        "                cleaned_content = ''.join(filter(str.isalpha, content))\n",
        "                sequences.append(cleaned_content.upper())  # Convert to upper case if needed\n",
        "    return sequences\n",
        "\n",
        "# Load training, validation, and test sets\n",
        "train_data = load_dna_sequences('train_data')\n",
        "train_labels = load_dna_sequences('train_labels')\n",
        "val_data = load_dna_sequences('val_data')\n",
        "val_labels = load_dna_sequences('val_labels')\n",
        "test_data = load_dna_sequences('test_data')\n",
        "test_labels = load_dna_sequences('test_labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjMiqBBDSas3"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hyx66R2ZWrrz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "class DNADataset(Dataset):\n",
        "    def __init__(self, data, labels, tokenizer):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_sequence = self.data[idx]\n",
        "        target_sequence = self.labels[idx]\n",
        "\n",
        "        input_tokens = self.tokenizer(input_sequence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "        target_tokens = self.tokenizer(target_sequence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "        return input_tokens.input_ids.squeeze(), target_tokens.input_ids.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qy_VlCRYDD8",
        "outputId": "cb015c97-4ccb-4c79-8816-e9dde7558bb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed\n",
            "Validation loss: 0.03717063601422859\n",
            "Epoch 1 completed\n",
            "Validation loss: 0.02943184882353403\n",
            "Epoch 2 completed\n",
            "Validation loss: 0.027702374722330683\n",
            "Epoch 3 completed\n",
            "Validation loss: 0.025820320496629728\n",
            "Epoch 4 completed\n",
            "Validation loss: 0.024015144680896282\n",
            "Epoch 5 completed\n",
            "Validation loss: 0.023865796205140406\n",
            "Epoch 6 completed\n",
            "Validation loss: 0.023635614036098367\n",
            "Epoch 7 completed\n",
            "Validation loss: 0.023333009450365545\n",
            "Epoch 8 completed\n",
            "Validation loss: 0.02243391638560417\n",
            "Epoch 9 completed\n",
            "Validation loss: 0.022137515109773773\n"
          ]
        }
      ],
      "source": [
        "# Prepare Datasets\n",
        "train_dataset = DNADataset(train_data, train_labels, tokenizer)\n",
        "val_dataset = DNADataset(val_data, val_labels, tokenizer)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=5)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(10):  # Number of epochs\n",
        "    model.train()\n",
        "    for input_ids, labels in train_loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} completed\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in val_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "    print(f\"Validation loss: {val_loss / len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1HNhrxoGGmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6594f4-f21d-47bd-903f-afb0126f213b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch num: 0\n",
            "Batch num: 1\n",
            "Batch num: 2\n",
            "Batch num: 3\n",
            "Batch num: 4\n",
            "Batch num: 5\n",
            "Batch num: 6\n",
            "Batch num: 7\n",
            "Batch num: 8\n",
            "Batch num: 9\n",
            "Batch num: 10\n",
            "Batch num: 11\n",
            "Batch num: 12\n",
            "Batch num: 13\n",
            "Batch num: 14\n",
            "Batch num: 15\n",
            "Batch num: 16\n",
            "Batch num: 17\n",
            "Batch num: 18\n",
            "Batch num: 19\n",
            "Batch num: 20\n",
            "Batch num: 21\n",
            "Batch num: 22\n",
            "Batch num: 23\n",
            "Batch num: 24\n",
            "Batch num: 25\n",
            "Batch num: 26\n",
            "Batch num: 27\n",
            "Batch num: 28\n",
            "Batch num: 29\n",
            "Batch num: 30\n",
            "Batch num: 31\n",
            "Batch num: 32\n",
            "Batch num: 33\n",
            "Batch num: 34\n",
            "Batch num: 35\n",
            "Batch num: 36\n",
            "Batch num: 37\n",
            "Batch num: 38\n",
            "Batch num: 39\n",
            "Batch num: 40\n",
            "Batch num: 41\n",
            "Batch num: 42\n",
            "Batch num: 43\n",
            "Batch num: 44\n",
            "Batch num: 45\n",
            "Batch num: 46\n",
            "Batch num: 47\n",
            "Batch num: 48\n",
            "Batch num: 49\n",
            "Batch num: 50\n",
            "Batch num: 51\n",
            "Batch num: 52\n",
            "Batch num: 53\n",
            "Batch num: 54\n",
            "Batch num: 55\n",
            "Batch num: 56\n",
            "Batch num: 57\n",
            "Batch num: 58\n",
            "Batch num: 59\n",
            "Batch num: 60\n",
            "Batch num: 61\n",
            "Batch num: 62\n",
            "Batch num: 63\n",
            "Batch num: 64\n",
            "Batch num: 65\n",
            "Batch num: 66\n",
            "Batch num: 67\n",
            "Batch num: 68\n",
            "Batch num: 69\n",
            "Batch num: 70\n",
            "Batch num: 71\n",
            "Batch num: 72\n",
            "Batch num: 73\n",
            "Batch num: 74\n",
            "Batch num: 75\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "test_dataset = DNADataset(test_data, test_labels, tokenizer)\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "\n",
        "# Using DNADataset for test data\n",
        "test_dataset = DNADataset(test_data, test_labels, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=5)  # Batch size set to 1 for individual prediction\n",
        "\n",
        "# Generate predictions\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "counter = 0\n",
        "with torch.no_grad():\n",
        "    for input_ids, _ in test_loader:\n",
        "        print(\"Batch num:\", counter)\n",
        "        input_ids = input_ids.to(model.device)\n",
        "        output = model.generate(input_ids, max_length=96, min_length=96, pad_token_id=tokenizer.pad_token_id)\n",
        "        prediction = tokenizer.decode(output[0], skip_special_tokens=True, trim_offsets=False)\n",
        "        predictions.append(prediction)\n",
        "        counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8StpVG2Ss0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d55100d-9a6b-4f42-e75d-355da192c2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Edit Distance: 0.023447141094199916\n"
          ]
        }
      ],
      "source": [
        "import editdistance\n",
        "\n",
        "# Assuming 'predictions' and 'test_labels' are lists of strings with your predicted and actual sequences\n",
        "\n",
        "def calculate_normalized_edit_distance(predictions, true_labels):\n",
        "    total_distance = 0\n",
        "    total_length = 0\n",
        "\n",
        "    for pred, true in zip(predictions, true_labels):\n",
        "        # Compute the edit distance for each pair of sequences\n",
        "        distance = editdistance.eval(pred[:96], true)\n",
        "        total_distance += distance\n",
        "        total_length += len(true)\n",
        "\n",
        "    # Normalizing the total edit distance by the total length of all sequences\n",
        "    avg_normalized_distance = total_distance / total_length\n",
        "    return avg_normalized_distance\n",
        "\n",
        "actuals = test_labels[::5]\n",
        "inputs = test_labels[::5]\n",
        "\n",
        "# Calculate the average edit distance\n",
        "average_edit_distance = calculate_normalized_edit_distance(predictions, actuals)\n",
        "print(\"Average Edit Distance:\", average_edit_distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYu0c8BeTwsL",
        "outputId": "84b74f7b-6fd8-473e-bb9e-ca410afad41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCAGATAAGGCCGTCACTCCGCAGAGTAGTCTGTTAACGCTTTCATTCCAGGACCCATTCATTGCGGTAGCGCCATCTCTTCAGTCTATCTTCACG\n",
            "GCAGATAAGGCCGTCACTCCGCAGAGTAGTCTGTTAACGCTTTCATTCCAGGACCCATTCATTGCGGTAGCGCCATCTCTTCAGTCCATCTTCACG\n",
            "GCAGATAAGGCCGTCACTCCGCAGAGTAGTCTGTTAACGCTTTCATTCCAGGACCCATTCATTGCGGTAGCGCCATCTCTTCAGTCCATCTTCACG\n"
          ]
        }
      ],
      "source": [
        "print(predictions[0][:96])\n",
        "print(inputs[0])\n",
        "print(actuals[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}